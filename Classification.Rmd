---
title: "ANLY-512: Final Project Classification Models"
author: "Kajal Tiwary"
output:
  html_document:
    df_print: paged
---

# Set seed to replicate results 
```{r}
## Set seed to last three digits GU ID for replication 
set.seed(755)
```

# Load all data 
```{r, warning=FALSE, message=FALSE}
## Load all necessary packages 
require(MASS)
require(glmnet)
library(kableExtra)
library(stargazer)
library(flextable)
library(dplyr)
library(leaps)
library(caret)
library(ggplot2)
library(ggExtra)
library(tidyr)
library(ISLR2)
library(ISLR)
library(readr)
library(reticulate)
library(randomForest) 
library("gridExtra") 
library(caret) 
library(ROCR)
library(pROC)
library(stepPlr)
library(doParallel) 
library(skimr)
library(gt)
library(corrplot)
library(GGally)
library(cvms)
library(tibble)
library(klaR)
library(adabag)
library(xgboost)
library(ipred)
library(e1071)
library(mlbench)
library(gbm)
```

# Load and prepare data 
```{r, warning=FALSE, message=FALSE}
setwd('/Users/kajaltiwary/Downloads/512-Group-Project-main/')

## Load dataset 
df <- read_csv('NBAdraft.csv')

## Remove all NA rows from the dataset 
df <- na.omit(df)

## Remove categorical variables from dataframe 
df <- df[ , !names(df) %in% c("School","name", "Rk")]

## Rename columns for ease of use later on 
names(df)[7] <- "FG_college_perc"
names(df)[10] <- "two_P_perc"
names(df)[9] <- "two_PA"
names(df)[8] <- "two_P"
names(df)[11] <- "three_P"
names(df)[12] <- "three_PA"
names(df)[13] <- "threeP_college_perc"
names(df)[16] <- "FT_college_perc"
```

# Create train and test set for models 
```{r, warning=FALSE, message=FALSE}
## Remove the Pk column from the data set 
df <- df[ , !names(df) %in% c("Pk")]

## Split the data into test and training sets 
row.train = sample(1:nrow(df),0.8*nrow(df))
df_train <- df[row.train, ]
df_test <- df[-row.train, ]

## Convert the lottery variable into a factor 
df_train$lottery<-as.factor(df_train$lottery)
df_test$lottery<-as.factor(df_test$lottery)
df$lottery <- as.factor(df$lottery)

## Convert lottery to correct factor format in training set 
df_train$lottery <- ifelse(df_train$lottery==TRUE,"lottery","non-lottery")
df_train$lottery <- as.factor(df_train$lottery)
df_train$lottery <- factor(df_train$lottery,levels=c("lottery","non-lottery"), labels=c("yes","no"))

## Convert lottery to correct factor format in test set 
df_test$lottery <- ifelse(df_test$lottery==TRUE,"lottery","non-lottery")
df_test$lottery <- as.factor(df_test$lottery)
df_test$lottery <- factor(df_test$lottery,levels=c("lottery","non-lottery"), labels=c("yes","no"))

## Convert lottery to correct factor format in original set 
df$lottery <- ifelse(df$lottery==TRUE,"lottery","non-lottery")
df$lottery <- as.factor(df$lottery)
df$lottery <- factor(df$lottery ,levels=c("lottery","non-lottery"), labels=c("yes","no"))
```

# Classification Model 1: Linear Discriminant Analysis (Cross-Validation)
```{r, fig.align='center', fig.height=7, fig.width=9}
tc <- trainControl(method = "cv", number = 5, summaryFunction = twoClassSummary, classProbs=TRUE, search="grid", returnData = TRUE, savePredictions = TRUE)

## Create an lda model using training set 
linear <- train(lottery ~., data=df, method="lda", trControl=tc)

## Extract the accuracy for each fold
accuracy_linear <- linear$resample
accuracy_linear$approach <- "lda"

## Plot the accuracy for each model 
accuracy_linear %>%
ggplot(aes(x = approach, y = ROC, fill=approach)) +
geom_boxplot() + ggtitle("Accuracy Distribution For LDA Model") + xlab("Model") + ylab("Accuracy (ROC)")

## Get the average accuracy for the model 
accuracy_linear_average <- accuracy_linear %>%
group_by(approach) %>%
summarise(mean_accuracy = mean(ROC))

## Extract the ROC standard deviations 
ROCSD <- linear$results$ROCSD
ROCSD <- as.data.frame(ROCSD)
ROCSD$model <- "lda"
linearROCSD <- ROCSD %>% rename(ROCSD = 1)
```

# Classification Model 2: Quadratic Discriminant Analysis (Cross-Validation)
```{r, fig.align='center', fig.height=7, fig.width=9}
## Create an lda model using training set 
quadratic <- train(lottery ~., data=df, method="qda", trControl=tc)

## Extract the accuracy for each fold
accuracy_quad <- quadratic$resample
accuracy_quad$approach <- "qda"

## Plot the accuracy for each model 
accuracy_quad %>%
ggplot(aes(x = approach, y = ROC, fill=approach)) +
geom_boxplot() + ggtitle("Accuracy Distribution For QDA Model") + xlab("Model") + ylab("Accuracy (ROC)")

## Get the average accuracy for the model 
accuracy_quad_average <- accuracy_quad %>%
group_by(approach) %>%
summarise(mean_accuracy = mean(ROC))

## Extract the Roc standard deviations 
ROCSD <- quadratic$results$ROCSD
ROCSD <- as.data.frame(ROCSD)
ROCSD$model <- "qda"
quadROCSD <- ROCSD %>% rename(ROCSD = 1)
```

# Classification Model 3: Logistic Model (Cross-Validation)
```{r, fig.align='center', fig.height=7, fig.width=9}
## Define cross-validated logistic model 
model.ch.1 <-train(lottery~., metric="ROC", method="glm", family="binomial", trControl=tc, data=df)

## Use glm to fit the same model 
glm.ch.1 <- glm(lottery ~., data = df, family = "binomial")

## Extract the values and print stargazer table 
model.ch.1$finalModel$call <- glm.ch.1$call

## Print out the model summary 
stargazer::stargazer(model.ch.1$finalModel, summary = TRUE, report = "vc*stp", ci = TRUE, type="text")

## Create a confusion matrix 
cm_log <- confusionMatrix(model.ch.1)

## Calculate and plot matrix 
conftab <- tibble(target=model.ch.1$pred$obs, prediction = model.ch.1$pred$pred)
basic_table <- table(conftab)
cfm <- as_tibble(basic_table)
## plot_confusion_matrix(cfm, target_col = "target", prediction_col = "prediction", counts_col="n")

## Overall accuracy 
log_acc<- sum(diag(basic_table))/sum(basic_table)

## Extract the accuracy for each fold / model 
accuracy_log <- model.ch.1$resample
accuracy_log$approach <- "logistic"

## Plot the accuracy for each model 
accuracy_log %>%
ggplot(aes(x = approach, y = ROC, fill=approach)) +
geom_boxplot() + ggtitle("Accuracy Distribution For Logistic Model") + xlab("Model") + ylab("Accuracy (ROC)")

## Get the average accuracy for the model 
accuracy_log_average <- accuracy_log %>%
group_by(approach) %>%
summarise(mean_accuracy = mean(ROC))

## Extract the Roc standard deviations 
ROCSD <- model.ch.1$results$ROCSD
ROCSD <- as.data.frame(ROCSD)
ROCSD$model <- "logistic"
logROCSD <- ROCSD %>% rename(ROCSD = 1)
```

# Classification Model 4: Random Forest Model (Cross-Validation)
```{r, fig.align='center', fig.height=7, fig.width=9}
## Use train to fit model to all the variables 
train_rf_model <- train(lottery~., metric = "ROC", method="rf", importance=T, proximity=F, trControl=tc, ntree=1000, sampsize= c(30,90), data = df)

## Use random forest to fit lottery to all variables in the data 
rf_model_two <- randomForest(lottery~., importance = TRUE, proximity=FALSE, confusion=TRUE,err.rate=TRUE,tuneLength = 10, data=df, ntree = 1000)

## Extract the accuracy for each fold / model 
accuracy_rf <- train_rf_model$resample
accuracy_rf$approach <- "random forest"

## Plot the accuracy for each model 
accuracy_rf %>%
ggplot(aes(x = approach, y = ROC, fill=approach)) +
geom_boxplot() + ggtitle("Accuracy Distribution For Each Random Forest Model") + xlab("Model") + ylab("Accuracy (ROC)")

## Get the average accuracy for the model 
accuracy_rf_mean <- accuracy_rf %>%
group_by(approach) %>%
summarise(mean_accuracy = mean(ROC))

## Create a confusion matrix 
cm_rf <- confusionMatrix(train_rf_model)

## Calculate and plot matrix 
conftab1 <- tibble(target=train_rf_model$pred$obs, prediction = train_rf_model$pred$pred)
basic_table1 <- table(conftab1)
cfm1 <- as_tibble(basic_table1)
## plot_confusion_matrix(cfm1, target_col = "target", prediction_col = "prediction", counts_col="n")

## Extract the Roc standard deviations 
ROCSD <- train_rf_model$results$ROCSD
ROCSD <- as.data.frame(ROCSD)
ROCSD$model <- "random forest"
rfROCSD <- ROCSD %>% rename(ROCSD = 1)
rfROCSD <- rfROCSD %>%
group_by(model) %>%
summarise(ROCSD = mean(ROCSD))
```

# Classification Model 5: Adaptive Boosting (Cross-Validation )
```{r, fig.align='center', fig.height=7, fig.width=9}
## Define grid and parameters for model 
gd <- expand.grid(nIter=seq(100,500,by=100), method="adaboost") 

## Develop model for five fold cross validation 
mod1_5 <- train(lottery~., metric = "ROC", method = 'adaboost', trControl=tc, data=df, tuneGrid = gd, tuneLength = 3)

## Extract the accuracy for each fold / model 
accuracy_ada <- mod1_5$resample
accuracy_ada$approach <- "adaptive boosting"

## Plot cross-fold accuracy 
accuracy_ada %>%
ggplot(aes(x = approach, y = ROC, fill=approach)) +
geom_boxplot() +ggtitle("Accuracy Distribution For Each Adaboost Model") + xlab("Model") + ylab("Accuracy (ROC)")

## Get the average accuracy for the model 
accuracy_ada$approach <- "adaptive boosting"
accuracy_ada_mean <- accuracy_ada %>%
group_by(approach) %>%
summarise(mean_accuracy = mean(ROC))

## Extract the Roc standard deviations 
ROCSD <- mod1_5$results$ROCSD
ROCSD <- as.data.frame(ROCSD)
ROCSD$model <- "adaptive boosting"
adaROCSD <- ROCSD %>% rename(ROCSD = 1)
adaROCSD <- adaROCSD %>%
group_by(model) %>%
summarise(ROCSD = mean(ROCSD))
```

# Classification Model 6: Bagging Model (Cross-Validation)
```{r, fig.align='center', fig.height=7, fig.width=9}
## Develop model for five fold cross validation 
mod3_5 <- train(lottery~., metric = "ROC", method = 'treebag', trControl=tc, control=rpart.control(minsplit=2), data=df, verbose=T)

## Extract the accuracy for each fold / model 
accuracy_bag <- mod3_5$resample
accuracy_bag$approach <-  "bagging"

## Plot the accuracy for each model 
accuracy_bag %>%
ggplot(aes(x = approach, y = ROC, fill=approach)) +
geom_boxplot() +ggtitle("Accuracy Distribution For Each Bagging Model") + xlab("Model") + ylab("Accuracy (ROC)")

## Get the average accuracy for the model 
accuracy_bag$approach <- "bagging"
accuracy_bag_mean <- accuracy_bag %>%
group_by(approach) %>%
summarise(mean_accuracy = mean(ROC))

## Extract the Roc standard deviations 
ROCSD <- mod3_5$results$ROCSD
ROCSD <- as.data.frame(ROCSD)
ROCSD$model <- "bagging"
bagROCSD <- ROCSD %>% rename(ROCSD = 1)
bagROCSD <- bagROCSD %>%
group_by(model) %>%
summarise(ROCSD = mean(ROCSD))
```

# Classification Model 7: Generalized Boosted Models (Cross-Validation)
```{r, fig.align='center', fig.height=7, fig.width=9}
## Define grid and parameters for model 
gd <- expand.grid(interaction.depth=c(1,3), n.trees = (0:50)*50, shrinkage=c(0.01, 0.001),  n.minobsinnode=10)

## Develop model for five fold cross validation 
mod2_5 <- train(lottery~., metric = "ROC", method = 'gbm', trControl=tc, data=df, tuneGrid=gd, tuneLength = 3,verbose=FALSE)

## Extract the accuracy for each fold / model 
accuracy_boost <- mod2_5$resample
accuracy_boost$approach <- "generalized boosted"

## Plot the accuracy for each model 
accuracy_boost %>%
ggplot(aes(x = approach, y = ROC, fill=approach)) +
geom_boxplot() +ggtitle("Accuracy Distribution For Each GBM Model") + xlab("Model") + ylab("Accuracy (ROC)")

## Get the average accuracy for the model 
accuracy_boost$approach <- "generalized boosted"
accuracy_boost_mean <- accuracy_boost %>%
group_by(approach) %>%
summarise(mean_accuracy = mean(ROC))

## Extract the Roc standard deviations 
ROCSD <- mod2_5$results$ROCSD
ROCSD <- as.data.frame(ROCSD)
ROCSD$model <- "generalized boosted"
gbmROCSD <- ROCSD %>% rename(ROCSD = 1)
gbmROCSD <- gbmROCSD %>%
group_by(model) %>%
summarise(ROCSD = mean(ROCSD))
```

# Plot the accuracies 
```{r chunk03, fig.align='center', fig.height=7, fig.width=9}
## Format each data frame before combining 
accuracy_boost <- accuracy_boost[c(1,5)]
accuracy_bag <- accuracy_bag[c(1,5)]
accuracy_ada <- accuracy_ada[c(1,5)]
accuracy_rf <- accuracy_rf[c(1,5)]
accuracy_log <- accuracy_log[c(1,5)]
accuracy_quad <- accuracy_quad[c(1,5)]
accuracy_linear <- accuracy_linear[c(1,5)]
accuracy_boost <- accuracy_boost %>% rename(ROC = 1)
accuracy_bag <- accuracy_bag %>% rename(ROC = 1)
accuracy_ada <- accuracy_ada %>% rename(ROC = 1)

## Combine all of the accuracy data frames together  
accuracy_df_final <- rbind(accuracy_boost, accuracy_bag, accuracy_ada, accuracy_rf, accuracy_log, accuracy_quad, accuracy_linear)

## Combine all of the accuracy average data frames together  
accuracy_df_final_avg <- rbind(accuracy_boost_mean, accuracy_bag_mean, accuracy_ada_mean, accuracy_rf_mean, accuracy_log_average, accuracy_quad_average, accuracy_linear_average)

## Round the average accuracy values to 5 digits 
accuracy_df_final_avg$mean_accuracy <- round(accuracy_df_final_avg$mean_accuracy,5)

## Combine all of the standard deviation data frames together 
sd_df_avg <- rbind(gbmROCSD, bagROCSD, adaROCSD,rfROCSD,logROCSD,quadROCSD,linearROCSD)

## Round the average standard deviation values to 5 digits 
sd_df_avg$ROCSD <- round(sd_df_avg$ROCSD,5)

## Plot the average standard deviation for each model  
plot3 <- ggplot(sd_df_avg, aes(x=model, y=ROCSD, fill=model)) + 
geom_bar(stat = "identity") + ggtitle("Average Standard Deviation For Each Classification Model") + xlab("Model") + ylab("Standard Deviation") +theme(axis.text.x = element_text(angle = 0)) + scale_fill_manual(values = c("#d19999","#ae4d4d","#8b0000", "#241aa5", "#08052f", "#404040", "#CFCFCF")) + geom_text(aes(label = ROCSD), size = 3, vjust=1.6, color="white")

## Plot the average ROC for each model 
plot2 <- ggplot(accuracy_df_final_avg, aes(x=approach, y=mean_accuracy, fill=approach)) + 
geom_bar(stat = "identity") + ggtitle("Average ROC For Each Classification Model") + xlab("Model") + ylab("ROC") +theme(axis.text.x = element_text(angle = 0)) + scale_fill_manual(values = c("#d19999","#ae4d4d","#8b0000", "#241aa5", "#08052f", "#404040", "#CFCFCF")) + geom_text(aes(label = mean_accuracy), size = 3, vjust=1.6, color="white")

## Plot the ROC distribution for each model 
plot1 <- accuracy_df_final %>%
ggplot(aes(x = approach, y = ROC, fill=approach)) +
geom_boxplot() +ggtitle("Cross-Validation ROC Distribution For Each Model") + xlab("Model") + ylab("ROC")+theme(axis.text.x = element_text(angle = 0)) + scale_fill_manual(values = c("#d19999","#ae4d4d","#8b0000", "#241aa5", "#08052f", "#404040", "#CFCFCF")) + theme(plot.title = element_text(size=22))  + theme(plot.title = element_text(size=22))

## Print the model comparison charts on one figure 
png("class_model_comparison.png", width = 800, height = 1000)
grid.arrange(plot1, plot2, plot3, ncol = 1)
dev.off()
```

## Get the most important variables for Generalized Boosting Model (best model)
```{r, fig.align='center', fig.height=7, fig.width=9}
## Extract the importance of each variable for generalized boosting model  
## importance_value <- mod2_5$importance
varimp <- varImp(mod2_5, scale=T)
varimp["FeatureName"] <- rownames(varimp)

## Extract the most important features 
var_imp <-data.frame(varImp(mod2_5)$importance)
var_imp$Feature<-row.names(var_imp)
df1 <- var_imp[order(-var_imp$Overall),][1:10,]

## Print the most important features 
most <- ggplot(df1, aes(x=reorder(Feature,Overall),y=Overall,fill=Overall))+geom_bar(stat="identity") +  ylab("Variable Importance") + xlab("Variable") + ggtitle("10 Most Important Variables")+scico::scale_fill_scico(palette = "vik")+theme(axis.text.x = element_text(angle = 90))

## Extract the least important features 
var_imp2 <-data.frame(varImp(mod2_5)$importance)
var_imp2$Feature<-row.names(var_imp2)
df2 <- var_imp2[order(var_imp2$Overall),][1:10,]

## Print the least important features 
least <- ggplot(df2, aes(x=reorder(Feature,Overall),y=Overall,fill=Overall))+geom_bar(stat="identity") +  ylab("Variable Importance") + xlab("Variable") + ggtitle("10 Least Important Variables")+scico::scale_fill_scico(palette = "vik")+theme(axis.text.x = element_text(angle = 90)) 

## Plot the feature importance on one figure 
png("class_feature_imp.png", width = 800, height = 600)
grid.arrange(most+coord_flip(), least+coord_flip(), nrow = 1)
dev.off()
```


